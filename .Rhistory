require(knitr)
# include this code chunk as-is to set options
opts_chunk$set(comment = NA, prompt = TRUE, tidy = FALSE, fig.width = 7, fig.height = 7,echo = TRUE, message = FALSE, warning = FALSE, cache=FALSE)
Sys.setlocale("LC_TIME", "C")
# load packages
if (!require(stringr)) install.packages("stringr", dep=TRUE)
if (!require(XML)) install.packages("XML", dep=TRUE)
if (!require(maps)) install.packages("maps", dep=TRUE)
require(stringr)
require(XML)
require(maps)
heritage_parsed <- htmlParse("https://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger", encoding = "UTF-8")
heritage_parsed <- htmlParse("worldheritagedanger.htm")
tables <- readHTMLTable(heritage_parsed, stringsAsFactors = FALSE)
names(tables)
danger_table <- tables[[2]]
heritage_parsed <- htmlParse("Exercises/worldHeritageDanger.html")
heritage_parsed <- XML::htmlParse("https://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger", encoding = "UTF-8")
heritage_parsed <- htmlParse("https://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger", encoding = "UTF-8")
tables <- readHTMLTable(heritage_parsed, stringsAsFactors = FALSE)
names(tables)
danger_table <- tables[[2]]
danger_table <- readHTMLTable(heritage_parsed, stringsAsFactors = FALSE, which = 2)
names(danger_table)
heritage_parsed <- htmlParse("https://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger")
heritage_parsed <- htmlParse("Exercises/worldheritagedanger.htm")
heritage_parsed <- htmlParse("worldheritagedanger.htm",encoding="UTF-8")
library(XML)
heritage_parsed <- htmlParse("worldheritagedanger.htm",encoding="UTF-8")
tables <- readHTMLTable(heritage_parsed, stringsAsFactors = FALSE)
danger_table <- tables[[2]]
tables
danger_table <- readHTMLTable(heritage_parsed, stringsAsFactors = FALSE, which = 2) # alternatively: directly select second table
heritage_parsed <- htmlParse("worldheritagedanger.htm",encoding="UTF-8")
heritage_parsed <- htmlParse("http://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger", encoding = "UTF-8")
heritage_parsed <- htmlParse("https://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger", encoding = "UTF-8")
heritage_parsed <- htmlParse("httpsÃ§://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger", encoding = "UTF-8")
heritage_parsed <- htmlParse("worldheritagedanger.htm",encoding="UTF-8")
library(RCurl)
fileURL <- "http://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger"
xData <- getURL(fileURL)
heritage_parsed <- htmlParse(xData)
heritage_parsed <- xmlParse(xData)
heritage_parsed <- xmlParse("worldheritagedanger.htm",encoding="UTF-8")
setwd("~/Dropbox (VHIR)/Cursos i Materials/Cursos/Web_Scrapping/aWebScrapping-Summer_School")
require(knitr)
# include this code chunk as-is to set options
opts_chunk$set(comment = NA, prompt = TRUE, tidy = FALSE, fig.width = 7, fig.height = 7,echo = TRUE, message = FALSE, warning = FALSE, cache=FALSE)
Sys.setlocale("LC_TIME", "C")
if (!require(twitteR)) install.packages("twitteR")
mytoken <-  "240357022-oBmZoFcJy7OpqLG1UNRDXavYtSxggcO2bnbIFQ77"    # "Access Token from Twitter"
key <- "XMAYl2tKiRYro43E9rSQG870Z"   # your Consumer Key (API Key)
secret <-   "a446H2hii2WZeTwXLQqnDpIL0dnRbauoCkgO4L7jsljt4cLEPR"  # Consumer Secret (API Secret)
mytoken <-  "240357022-oBmZoFcJy7OpqLG1UNRDXavYtSxggcO2bnbIFQ77"    # "Access Token from Twitter"
secretoken <- "duwWUjsaK4jIEePuIN6d9iOO9MJl06wj6M9QaO6MOjxfa"   # "Access Token Secret from Twitter"
require(twitteR)
setup_twitter_oauth(consumer_key = key, consumer_secret = secret,
access_token= mytoken, access_secret=secretoken)
searchTwitter("#beer", n=100)
searchTwitter("#beer", n=10)
require(twitteR)
search.string <- "#Trump"
no.of.tweets <- 1000
require(twitteR)
search.string <- "#Trump"
no.of.tweets <- 1000
myTweets <- searchTwitter(search.string, n=no.of.tweets,
since='2018-06-22', until='2018-06-28', lang="en")
head(myTweets)
save(myTweets, file="myTweets.Rda")
require(twitteR)
showTweets <- TRUE
if (!exists("myTweets")) load("myTweets.Rda")
tweets.text <- sapply(myTweets, function(x) x$getText())
tweets.text <- gsub("@\\w+", "", tweets.text)
tweets.text <- gsub("[[:punct:]]", "", tweets.text)
tweets.text <- gsub("http\\w+", "", tweets.text)
tweets.text <- gsub("[ |\t]{2,}", "", tweets.text)
tweets.text<-  gsub( '[^A-z0-9_]', ' ', tweets.text)
tweets.text <- tolower(tweets.text)
tweets.text <- gsub("rt", "", tweets.text)
tweets.text <- gsub("^ ", "", tweets.text)
tweets.text <- gsub(" $", "", tweets.text)
head(tweets.text)
require("tm")
if (!require(tm)) install.packages("tm")
require("tm")
tweets.text.corpus <- Corpus(VectorSource(tweets.text))
head(stopwords())
tweets.text.corpus <- tm_map(tweets.text.corpus, function(x) removeWords(x, stopwords()))
head(tweets.text.corpus)
tweets.text.corpus <- Corpus(VectorSource(tweets.text))
head(tweets.text.corpus)
wordcloud(tweets.text.corpus, min.freq = 2, scale=c(7,0.5),
colors=brewer.pal(8, "Dark2"),random.color= TRUE, random.order = FALSE,
max.words = 150)
require(wordcloud)
require(wordcloud)
if (! require(wordcloud) ) install.packages("wordcloud")
require(wordcloud)
wordcloud(tweets.text.corpus, min.freq = 2, scale=c(7,0.5),
colors=brewer.pal(8, "Dark2"),random.color= TRUE, random.order = FALSE,
max.words = 150)
require(tm)
mytdm <- TermDocumentMatrix(tweets.text.corpus)
inspect(mytdm[1:15,1:30])
findFreqTerms(mytdm, lowfreq=55) # experiment with the lowfreq
tdm <-removeSparseTerms(mytdm, sparse=0.93) # experimet with sparse
tdmscale <- scale(tdm)
dist <- dist(tdmscale, method = "canberra")
fit <- hclust(dist)
par(mai=c(1,1.2,1,0.5))
plot(fit, xlab="", sub="", col.main="salmon")
rect.hclust(fit, k=6, border="salmon")
pos <- readLines("Positive-Words.txt")
setwd("~/Dropbox (VHIR)/Cursos i Materials/Cursos/Web_Scrapping/aWebScrapping-Summer_School")
pos <- readLines("Positive-Words.txt")
pos <- readLines("Positive-Words.txt")
pos[sample(1:length(pos), 5)]
neg <- readLines("Negative-Words.txt")
neg[sample(1:length(neg), 5)]
if(!require(plyr)) install.packages("plyr")
source("https://raw.githubusercontent.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/master/R/sentiment.R")
sentiment
sentiment.R
View(score.sentiment)
require(twitteR)
usatweets = searchTwitter("usa", n=900, lang="en")
indiatweets = searchTwitter("india", n=900, lang="en")
russiatweets = searchTwitter("russia", n=900, lang="en")
chinatweets = searchTwitter("china", n=900, lang="en")
save(usatweets, indiatweets, russiatweets, chinatweets, file="countryTweets.Rda")
if (!exists("usatweets") )load(file="countryTweets.Rda")
usa_txt = sapply(usatweets, function(x) x$getText())
india_txt = sapply(indiatweets, function(x) x$getText())
russia_txt = sapply(russiatweets, function(x) x$getText())
china_txt = sapply(chinatweets, function(x) x$getText())
country = c(usa_txt, india_txt, russia_txt, china_txt)
countries <-  gsub( '[^A-z0-9_]', ' ', country)
scores <- score.sentiment(countries, pos, neg, .progress='none')
nd <- c(length(usa_txt), length(india_txt), length(russia_txt), length(china_txt))
scores$country = factor(rep(c("usa", "india", "russia", "china"), nd))
scores$very.pos = as.numeric(scores$score >= 2)
scores$very.neg = as.numeric(scores$score <= -2)
numpos = sum(scores$very.pos)
numneg = sum(scores$very.neg)
```{r}
head(scores)
boxplot(score~country, data=scores)
require(lattice)
histogram(data=scores, ~score|country,
main="Sentiment Analysis of 4 Countries", xlab="", sub="Sentiment Score")
