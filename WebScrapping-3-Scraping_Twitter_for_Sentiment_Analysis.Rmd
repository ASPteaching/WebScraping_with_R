---
title: "Web Scraping with R (3): <br> Scraping twiter for Sentiment Analysis"
author: "Alex Sanchez (asanchez@ub.edu) <br> GME Department. Universitat de Barcelona <br> Statistics and Bioinformatics Unit. Vall d'Hebron Institut de Recerca"
date: "XII MESIO Summer School (2018)"
output:
  slidy_presentation:
    fig_width: 7
    fig_height: 6
css: myStyles.css
footer: "Scrapping twiter"
keep_md: true
highlight: pygments
editor_options: 
  chunk_output_type: console
---


```{r setLicense, child = 'license.Rmd'}
```

```{r disclaimer, child = 'disclaimer.Rmd'}
```

```{r setup, include=FALSE}
require(knitr)
# include this code chunk as-is to set options
opts_chunk$set(comment = NA, prompt = TRUE, tidy = FALSE, fig.width = 7, fig.height = 7,echo = TRUE, message = FALSE, warning = FALSE, cache=FALSE)
Sys.setlocale("LC_TIME", "C")
```
Installing necessary R packages
====================================

- For the purpose of this chapter, we will need the following packages:
    + `ROAuth`: Provides an interface to the OAuth 1.0 specification, allowing users to authenticate via OAuth to the server of their choice.
    + `httr`: Useful tools for working with the HTTP protocol. allows alternative authentication.
    + `Twitter`: Provides an interface to the Twitter web API.
    + `tm`: Text mining package. 
- Start by installing and loading all the required packages.

```{r eval=TRUE}
if(!require(plyr)) install.packages("plyr")
if(!require(stringr)) install.packages("stringr")
if (!require(twitteR)) install.packages("twitteR")
# if (!require(ROAuth)) install.packages("ROAuth")
# if (!require(httr)) install.packages("httr")
if (!require(tm)) install.packages("tm")
if (! require(wordcloud) ) install.packages("wordcloud")
```

Introduction: Scraping Twitter for Sentiment Analysis
=====================================================

- Social networks like Twitter provide an excellent environment for web scraping:
    + They contain "interesting" data (e.g. *trending topics*, )
    + The data is not readily available, so getting it is a challenge in itself, but there are *packages and tools that can facilitate the process*.
    + The analysis of textual data extracted from Twitter requires techniques that are not common in the statistician's toolbox, so *an introduction to Textual and to Sentiment Analysis* will be offered.
    
How can we analyze natural language (text)
============================================

- Textual analysis is an [old friend](http://www.raco.cat/index.php/anuariopsicologia/article/viewFile/61168/88733) of statistics, especially in the french school, especially in social sciences (i.e. analysis of answers to open questions in surveys).
- R's [Natural Language Processing Task View](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html)  collects R packages to support conducting analysis of *speech* and *language* on a variety of levels. 
- [**`tm`**](https://cran.r-project.org/web/packages/tm/index.html), "Text Mining Package" is one of the main packages available for this type of tasks.
- **`tm`** incorporates functions for reading text corpus and "breaking" them in different ways that facilitate their analysis 
    + most of the times it starts by some type cleaning followed by 
    + the analysis of diverse frequency tabulations.
- A simple case study can be found here: [https://jhuria.wordpress.com/2012/07/01/text-mining-in-r/](https://jhuria.wordpress.com/2012/07/01/text-mining-in-r/)


Linking Text Mining and Web Scraping
==========================================

- Text mining can be done, of course, in a web-independent manner.
- Alternatively, if one takes into account that the web is populated with all types of (unstructured) text, the relation becomes obvious.
- See the "Social Media Clients" section in the Web Technologies Task View in CRAN ([Site](https://cran.r-project.org/web/views/WebTechnologies.html), [Description](https://journal.r-project.org/archive/2014-1/mair-chamberlain.pdf)) for a variety of packages that facilitate the connection with diverse source of data, such as Google, Facebook, Twitter, or Amazon which provide APIs which allow analysts to retrieve data.

Preparing data acquisition from twitter
============================================

- In order to extract tweets, one needs a *Twitter application* and hence a *Twitter account*. If you don’t have a Twitter account, please sign up.
- Use your Twitter login ID and password to sign in at [Twitter Developers](https://apps.twitter.com/).
- Once you have signed in both accounts (Twitter --> Twitter developers) you can start to *"Create New App"*
    + Provide the necessary details (notice that some are compulsory and some are not)
    + Agree to the developer Agreement
- As you have created this app you have been provided with a series of keys, accessible from the "Keys and Tokens", tab that **need to be used** to  authenticate, that is grant you access, so that you can retrieve bunchs of tweets.
 
<!-- Authentication (1): Obtain credentials -->
<!-- ======================================================== -->

<!-- - In order to connect with Twitter from a Windows system we need to download a  a bunch of certificates that allow the system to confirm that the site where you are connecting is where you really intend to connect. -->

<!-- ```{r eval=FALSE} -->
<!-- require(ROAuth) -->
<!-- # Download "cacert.pem" file -->
<!-- download.file(url="http://curl.haxx.se/ca/cacert.pem",destfile="cacert.pem") -->
<!-- ``` -->
<!-- Next we create an object "cred" that will save the authenticated object that we can use for later sessions. -->

<!-- ```{r eval=FALSE} -->
<!-- cred <- OAuthFactory$new(consumerKey='XMAYl2tKiRYro43E9rSQG870Z', -->
<!--       consumerSecret='a446H2hii2WZeTwXLQqnDpIL0dnRbauoCkgO4L7jsljt4cLEPR', -->
<!--       requestURL='https://api.twitter.com/oauth/request_token', -->
<!--       accessURL='https://api.twitter.com/oauth/access_token', -->
<!--       authURL='https://api.twitter.com/oauth/authorize') -->
<!-- ``` -->

<!-- Authentication (2): Use credentials -->
<!-- ======================================================== -->
<!-- - Executing the next step generates the following output:  -->
<!--     + *To enable the connection, please direct your web browser to: "hyperlink" . Note:  You only need to do this part once.* -->

<!-- ```{r eval=FALSE} -->
<!--  cred$handshake(cainfo="cacert.pem") -->
<!--  #save for later use for Windows -->
<!--  save(cred, file="twitter authentication.Rdata") -->
<!-- ``` -->
<!-- - When the previous step is completed we have generated "twitter authentication credentials" that we will use for mutual identification between twitter an ourselves. -->

<!-- If this process works well we shall be able to conect with the twitter API and extract information from tweets -->


Authentication (1): Access tokens
=================================

- In order to be able to recover tweets "massively" we nned to ba granted access, whas is described as "authentication".
- For this you need the following objects from the "keys and access token" tab in your developers account
    + key <- "your API key from twitter"
    + secret <- "your Secret key from twitter"
    + secrettk <- "Access Token Secret from Twitter"
    + mytoken <- "Access Token from Twitter"
    
```{r eval=FALSE}
# Use this code chunk to assign values to these 4 variables.
# The values can be obtained from **your** twitter developer account.
key <- XXXX         # "your API key from twitter"
secret <-   XXXX  # "your Secret key from twitter"
mytoken <-  XXXX    # "Access Token from Twitter"
secrettk <- XXXX   # "Access Token Secret from Twitter"
```

```{r eval=TRUE, results='hide', echo=FALSE, include=TRUE}
source("twitterAutentication.R") # Do not put thhis file in github (exclued it with .gitignore)
```
    

Authentication (2)
====================

- For the authentication we will also use the `setup_twitter_oauth` function (`?setup_twitter_oauth`)

```{r  eval=TRUE}
require(twitteR)
setup_twitter_oauth(consumer_key = key, consumer_secret = secret,
                    access_token= mytoken, access_secret=secretoken)
 # (1) choose direct authentication and answer "Yes"
```

This should let you connected so that you can start searching twitter.

The connection can be tested with a simple search whose meaning will be explained in the following slides.

```{r}
searchTwitter("#beer", n=10)
```

If something is returned it means the system is working.


Package overview
=================

- The [`twitteR` vignette](geoffjentry.hexdump.org/twitteR.pdf) provides an overview of the main functionalities of the package:
 
 1. Search Twitter: `searchTwitter()`
 2. Look at users : `getUser()`
 3. Manipulate grabbed tweets (convert to data.frames):  `twListToDF()`
 4. Timelines (streams of tweets):  `userTimeline()`
 5. Explore Trends:  `getTrends()`


An overview of the process
===================================================

- A typical information extraction process may be the following
1. Extract tweets and followers from the Twitter website with R
and the `twitteR` package.
2. Clean text by removing punctuations, numbers, hyperlinks and stop words.
- This will be followed by *a variety* of possible analyses:
3. Build a term-document matrix and make a Word Cloud.
4. Analyse topics with the `topicmodels` package.
5. Analyse sentiment with the `sentiment140` package.
6. Analyse following/followed and retweeting relationships with the `igraph` package

Extract tweets
===================

- Start setting twitter and authenticating as described in the previous steps.

- Information acquisition with the `twitteR` package is based on the `searchTwitter` function.
- It is recommended to read the official Twitter Search API documentation (https://dev.twitter.com/rest/public/search)[https://dev.twitter.com/rest/public/search]
- You will find things such as: *The Twitter Search API searches against a sampling of recent Tweets published in the past 7 days*. So do not try to llok for tweets older than a week!

```{r eval=FALSE}
require(twitteR)
search.string <- "#Trump"
no.of.tweets <- 1000
myTweets <- searchTwitter(search.string, n=no.of.tweets, 
                        since='2018-06-22', until='2018-06-28', lang="en")
head(myTweets)
save(myTweets, file="myTweets.Rda")
```


Clean up text (1) 
=================

- The product of the scraping process is a list with the "raw" content of each tweet.
- Before we do any analysis we usually want to clean the text.
- Although this may be done using the `tm` package it may be clearer to understand if we "simply" rely on regular expressions.


Code for cleaning tweets
===========================

```{r regexClean}
require(twitteR)
showTweets <- TRUE
if (!exists("myTweets")) load("myTweets.Rda")
 tweets.text <- sapply(myTweets, function(x) x$getText())
 # Replace @UserName
 tweets.text <- gsub("@\\w+", "", tweets.text)
 # Remove punctuation
 tweets.text <- gsub("[[:punct:]]", "", tweets.text)
 # Remove links
 tweets.text <- gsub("http\\w+", "", tweets.text)
 # Remove tabs
 tweets.text <- gsub("[ |\t]{2,}", "", tweets.text)
 # remove codes that are neither characters nor digits
 tweets.text<-  gsub( '[^A-z0-9_]', ' ', tweets.text)
 # Set characters to lowercase
 tweets.text <- tolower(tweets.text)
# Replace blank space (“rt”)
tweets.text <- gsub("rt", "", tweets.text)
 # Remove blank spaces at the beginning
 tweets.text <- gsub("^ ", "", tweets.text)
# Remove blank spaces at the end
 tweets.text <- gsub(" $", "", tweets.text)
 head(tweets.text)
```

Clean the text (2): Remove "common words"
=========================================

- Once the text has been cleaned it may be "pruned" from words that are known to be non-relevant.
- An example are commonly used words such as "a" or "the". 
- Such terms are known as "Stop words" and we usually obtain them from some place where they have been compiled.
- The `tm` package contains an instance of such list that can be recovered with the `stopwords` function.
- The `tm` package represents texts as *collections of documents* which are called "corpus".

Code for removing `stopwords`
==============================

```{r}
require("tm")
#create corpus
tweets.text.corpus <- Corpus(VectorSource(tweets.text))
head(stopwords())
#clean up by removing stop words
tweets.text.corpus <- tm_map(tweets.text.corpus, function(x) removeWords(x, stopwords()))
head(tweets.text.corpus)
```

Textual Analysis (1) Visualize tweets as Word Cloud
=============================================

- A simple visualization can be obtained using  a "cloud of words".
- This can be done with the `wordcloud` package.


```{r message=FALSE}
#generate wordcloud
require(wordcloud)
wordcloud(tweets.text.corpus, min.freq = 2, scale=c(7,0.5),
colors=brewer.pal(8, "Dark2"),random.color= TRUE, random.order = FALSE, 
max.words = 150)
```

Textual Analysis (2) Visualize counts and groups
=============================================

- A more classical visualization can also be obtained if one does a *simple* frequency analysis.

```{r}
require(tm)
#create corpus
# mycorpus <- Corpus(VectorSource(tweets.text))
# mytdm <- TermDocumentMatrix(mycorpus)
mytdm <- TermDocumentMatrix(tweets.text.corpus)
inspect(mytdm[1:15,1:30])
findFreqTerms(mytdm, lowfreq=55) # experiment with the lowfreq
tdm <-removeSparseTerms(mytdm, sparse=0.93) # experimet with sparse
```

Textual Analysis (2) Visualize counts and groups
=============================================
- Using the frequency counts obtained from the text corpus we can build a distance matrix and plot a dendrogram.

```{r}
tdmscale <- scale(tdm)
dist <- dist(tdmscale, method = "canberra")
fit <- hclust(dist)

# we need to change the margins and delete some titles
par(mai=c(1,1.2,1,0.5))
plot(fit, xlab="", sub="", col.main="salmon")
rect.hclust(fit, k=6, border="salmon")
# cutree(fit, k=6)
```

Sentiment Analysis
====================

- Sentiment analysis is used to see if a text is neutral, positive or negative.
- Emotion analysis is used to see which emotion a text has (happy, fear, anger) 
- Both are using similar codes but the comparison lexicon is different.
    + Example: What is the sentiment towards my company?
    
- Twitter data is useful for that type of analysis because:
    + high volumes (500 mill/day)
    + short messages like sms - 140 words
    + special strings (hashtags)
    + but creative word usage makes it hard for analysis, spelling mistakes
- There is **TONS** of sentiment in it!

Sentiment lexicons
===================

- Sentiment Lexicon: a list of words which you are using to compare your scraped text with.
- There exists a "standard list of words for sentiment analysis" compiled by Hu liu
- Essentially it is a list of positive and negative words created manually - approx. 6800
- Easy to find in te web but available in Atenea

```{r}
pos <- readLines("Positive-Words.txt")
pos[sample(1:length(pos), 5)]
neg <- readLines("Negative-Words.txt")
neg[sample(1:length(neg), 5)]
```

Scoring function
===================

- Positive and negative words may be scored using a simple ad-hoc function such as `score.sentiment` written by Jeff Breen.
- See its github repo on Sentiment analysis with R, [here](https://github.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107)

```{r}
source("https://raw.githubusercontent.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/master/R/sentiment.R")
```

Give a look at function `score.sentiment` to notice how simple is the idea for scoring sentiments.

Using the function
===================

- We are going to do sentiment analysis for a list of words that come from 4 tweeter searches.

```{r eval=FALSE}
require(twitteR)
usatweets = searchTwitter("usa", n=900, lang="en")
indiatweets = searchTwitter("india", n=900, lang="en")
russiatweets = searchTwitter("russia", n=900, lang="en")
chinatweets = searchTwitter("china", n=900, lang="en")
save(usatweets, indiatweets, russiatweets, chinatweets, file="countryTweets.Rda")
```

Exploring country tweets
==========================

- We start getting the text and joining it into a single character vector

```{r}
if (!exists("usatweets") )load(file="countryTweets.Rda")
usa_txt = sapply(usatweets, function(x) x$getText())
india_txt = sapply(indiatweets, function(x) x$getText())
russia_txt = sapply(russiatweets, function(x) x$getText())
china_txt = sapply(chinatweets, function(x) x$getText())
country = c(usa_txt, india_txt, russia_txt, china_txt)
```

Applying the `score.sentiment` function
=========================================

```{r}
countries <-  gsub( '[^A-z0-9_]', ' ', country)
scores <- score.sentiment(countries, pos, neg, .progress='none')
nd <- c(length(usa_txt), length(india_txt), length(russia_txt), length(china_txt))
scores$country = factor(rep(c("usa", "india", "russia", "china"), nd))
scores$very.pos = as.numeric(scores$score >= 2)
scores$very.neg = as.numeric(scores$score <= -2)
numpos = sum(scores$very.pos)
numneg = sum(scores$very.neg)
```

Exploring the scores (1)
======================

```{r}
head(scores)
boxplot(score~country, data=scores)
```

Exploring the scores (2)
=========================

```{r}
require(lattice)
histogram(data=scores, ~score|country, 
main="Sentiment Analysis of 4 Countries", xlab="", sub="Sentiment Score")

```


