---
title: "Parsing HTML with the `rvest`"
author: "Alex Sanchez"
date: "June 2018"
output: 
  html_document:
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, echo=FALSE}
require(knitr)
# include this code chunk as-is to set options
opts_chunk$set(comment = NA, prompt = TRUE, tidy = FALSE, fig.width = 7, fig.height = 7,echo = TRUE, message = FALSE, warning = FALSE, cache=FALSE)
Sys.setlocale("LC_TIME", "C")
```

```{r, echo=FALSE, message=FALSE}
# load packages
if (!require(rvest)) install.packages("rvest", dep=TRUE)
require(rvest)
```

# Scraping exercises

Remember

1. Start any exercise looking the website you are asked to scrap.
2. Devise your scraping strategy
3. Execute
4. Check the consistency of what you have obtained. Clean it whenever needed
5. Eventually analyze the data

## Exercise 1

Consider the url 'https://statbel.fgov.be/en/themes/indicators/prices/service-price-indices#panel-11'

Extract all the information load on table.

### Solution 1

```{r}
require(rvest)
url<- 'https://statbel.fgov.be/en/themes/indicators/prices/service-price-indices#panel-11'
# url='http://statbel.fgov.be/en/statistics/figures/economy/indicators/prix_prod_con/'

TAB <- read_html(url) %>% html_nodes('td') %>% html_text()
NAMES <- read_html(url)%>% html_nodes('th') %>% html_text()
M <- data.frame(matrix(TAB, ncol=5, nrow=13, byrow=T))
M  <- rbind(rep(NA,5),M)
M <- cbind(NAMES[7:20],M)
names(M) = NAMES[1:6]
```
An alernative approch is using `html_table` function

```{r}
require(rvest)
url<- 'https://statbel.fgov.be/en/themes/indicators/prices/service-price-indices#panel-11'
page <- read_html (url) 
myTables <-html_table(page, fill = TRUE) 
M2<- myTables[[1]]
```
The process seems easier to do using `html_table`

## Exercise 2

Consider the url 'http://www2.sas.com/proceedings/sugi30/toc.html'

Extract all the papers names, from 001-30 to 268-30

HINT: Use selectorgagdget to see that selector `cite` is associated with the paper titles.

```{r eval=FALSE}
url<- 'http://www2.sas.com/proceedings/sugi30/toc.html'
names <- read_html(url) %>% 
  html_nodes('cite') %>% 
  html_text() # html_attr('href')
names[1:6]

```

```{r eval=FALSE}
url<- 'http://www2.sas.com/proceedings/sugi30/toc.html'
urls <- read_html(url) %>% 
  html_nodes('cite') %>% 
  html_attr('href')
urls[1:6]

```


## Exercise 3

Consider the url 'http://www.gibbon.se/Retailer/Map.aspx?SectionId=832'

Extract all the options (Countries) availables on select button.

```{r eval=FALSE}
url <- 'http://www.gibbon.se/Retailer/Map.aspx?SectionId=832'
countries <- read_html (url)  %>%
  html_nodes('#ctl00_ContentPlaceHolder1__countries') %>% 
  html_children() %>% 
  html_text()
countries
```

Notice that once we have the right node we access the text in children nodes using `html_children`

## Exercise 4

Consider the url 'http://r-exercises.com/start-here-to-learn-r/'

Extract *all the topics* available on the url.

- Looking at the page contents we realize that "topics" appear in bold while subtopics don't.
- We use this characteristic to make three _nested_ calls to `html_nodes()` in order to recover these "artificial" subheadings 
- _the process would have been easier if the page developer had decided to give these headers a format using css but it's her/his decision_

```{r eval=FALSE}
url <- 'http://www.r-exercises.com/start-here-to-learn-r/'
topics <- read_html(url) %>% 
  html_nodes('p') %>% 
  html_nodes('strong') %>% 
  html_nodes('a') %>% 
  html_text()
topics[1:20]
```

## Exercise 5

Consider the url 'http://www.immobiliare.it/Roma/agenzie_immobiliari_provincia-Roma.html'

Extract all inmobiliaries names published on first page.

```{r eval=FALSE}
url<- 'http://www.immobiliare.it/Roma/agenzie_immobiliari_provincia-Roma.html'
names <- read_html(url) %>% 
  html_nodes('h4') %>% 
  html_nodes('a') %>% 
  html_text()
```


## Exercise 6

Consider the url='http://www.dictionary.com/browse/' and the words 'handy','whisper','lovely','scrape'.

Build a data frame, where the first variables is "Word" and the second variables is "definitions". Scrape the definitions from the url.

_This exercise does not work  in its current form because the page has been changed_

```{r eval=FALSE}
word=c('handy','whisper','lovely','scrape')
defs=c()
for(i in 1:4)
{
url=paste('http://www.dictionary.com/browse/',word[i],sep="")
a=read_html(url)%>%html_nodes('.def-content,#source-word-origin')%>%html_text()
def=a[1:grep(paste('Origin of ',word[i],sep=''),a)-1]
def=unlist(strsplit(def,'\r'))
def=unique(unlist(strsplit(def,'\n')))
def=paste(def,rep('/',length(def)))
def=toString(def)
defs=c(defs,def)
}
s=data.frame(cbind(word,defs))
s[1:2,] 
```


