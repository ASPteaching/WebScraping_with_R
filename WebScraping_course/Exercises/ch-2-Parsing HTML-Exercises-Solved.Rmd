---
title: "Parsing HTML with the `rvest`"
author: "Alex Sanchez"
date: "June 2018"
output: 
  html_document:
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, echo=FALSE}
require(knitr)
# include this code chunk as-is to set options
opts_chunk$set(comment = NA, prompt = TRUE, tidy = FALSE, fig.width = 7, fig.height = 7,echo = TRUE, message = FALSE, warning = FALSE, cache=FALSE)
Sys.setlocale("LC_TIME", "C")
```

```{r, echo=FALSE, message=FALSE}
# load packages
if (!require(rvest)) install.packages("rvest", dep=TRUE)
require(rvest)
```

# Scraping exercises

Remember

1. Start any exercise looking the website you are asked to scrap.
2. Devise your scraping strategy
3. Execute
4. Check the consistency of what you have obtained. Clean it whenever needed
5. Eventually analyze the data

## Exercise 1

Consider the url 'https://statbel.fgov.be/en/themes/indicators/prices/service-price-indices#panel-11'

Extract all the information load on table.

### Solution 1

```{r}
library(rvest)
url<- 'https://statbel.fgov.be/en/themes/indicators/prices/service-price-indices#panel-11'
# url='http://statbel.fgov.be/en/statistics/figures/economy/indicators/prix_prod_con/'

TAB <- read_html(url) %>% html_nodes('td') %>% html_text()
NAMES <- read_html(url)%>% html_nodes('th') %>% html_text()
M <- data.frame(matrix(TAB, ncol=5, nrow=13, byrow=T))
M  <- rbind(rep(NA,5),M)
M <- cbind(NAMES[7:20],M)
names(M) = NAMES[1:6]
```

## Exercise 2

Consider the url 'http://www2.sas.com/proceedings/sugi30/toc.html'

Extract all the papers names, from 001-30 to 268-30

HINT: Use selectorgagdget to see that selector `cite` is associated with the paper titles.

```{r eval=FALSE}
url='http://www2.sas.com/proceedings/sugi30/toc.html'
names=read_html(url)%>%html_nodes('cite')%>%html_text()
names[1:6]

```



## Exercise 3

Consider the url 'http://www.gibbon.se/Retailer/Map.aspx?SectionId=832'

Extract all the options (Countries) availables on select button.

```{r eval=FALSE}
url='http://www.gibbon.se/Retailer/Map.aspx?SectionId=832'
countries=read_html(url)%>%html_nodes('#ctl00_ContentPlaceHolder1__countries')%>%html_children()%>%html_text()
countries
```

## Exercise 4

Consider the url 'http://r-exercises.com/start-here-to-learn-r/'

Extract *all the topics* available on the url.

```{r eval=FALSE}
url='http://www.r-exercises.com/start-here-to-learn-r/'
topics=read_html(url)%>%html_nodes('a')%>%html_text()
topics[23:33]
```

## Exercise 5

Consider the url 'http://www.immobiliare.it/Roma/agenzie_immobiliari_provincia-Roma.html'

Extract all inmobiliaries names published on first page.

```{r eval=FALSE}
url='http://www.immobiliare.it/Roma/agenzie_immobiliari_provincia-Roma.html'
names=read_html(url)%>%html_nodes('a')%>%html_text()
First=grep('UEMME',names)
Last=max(grep('RE/MAX',names))
LIST=unique(names[First:Last])
LIST=LIST[-2]
LIST=LIST[-2]
LIST[1:5]
```


## Exercise 6

Consider the url='http://www.dictionary.com/browse/' and the words 'handy','whisper','lovely','scrape'.

Build a data frame, where the first variables is "Word" and the second variables is "definitions". Scrape the definitions from the url.

```{r eval=FALSE}
word=c('handy','whisper','lovely','scrape')
defs=c()
for(i in 1:4)
{
url=paste('http://www.dictionary.com/browse/',word[i],sep="")
a=read_html(url)%>%html_nodes('.def-content,#source-word-origin')%>%html_text()
def=a[1:grep(paste('Origin of ',word[i],sep=''),a)-1]
def=unlist(strsplit(def,'\r'))
def=unique(unlist(strsplit(def,'\n')))
def=paste(def,rep('/',length(def)))
def=toString(def)
defs=c(defs,def)
}
s=data.frame(cbind(word,defs))
s[1:2,] 
```


