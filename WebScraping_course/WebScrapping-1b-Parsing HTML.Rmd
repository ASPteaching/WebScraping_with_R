---
title: "Web Scraping with R (1): Parsing HTML"
author: "Alex Sanchez (asanchez@ub.edu) <br> GME Department. Universitat de Barcelona <br> Statistics and Bioinformatics Unit. Vall d'Hebron Institut de Recerca"
date: "April 2019"
output:
  slidy_presentation:
    fig_width: 7
    fig_height: 6
css: myStyles.css
footer: "Parsing HTML"
keep_md: true
highlight: pygments
editor_options: 
  chunk_output_type: console
---

```{r setLicense, child = 'license.Rmd'}
```

```{r disclaimer, child = 'disclaimer.Rmd'}
```

Introduction
=============
- In web scraping, we usually get in touch with HTML in two steps: 
    + First, we inspect content on the Web and examine whether it is attractive for further analyses.
    + Second, we import HTML files into R and extract information from them. 

- Parsing HTML occurs at both steps
    + *by the browser* to display HTML content nicely, and also  
    + *by parsers in R* to construct useful representations of HTML documents in our programming environment.

What is *parsing*
=================
- Parsing involves *breaking down a text into its component parts of speech with an explanation of the form, function, and syntactic relationship of each part*. [Wikipedia](https://en.wikipedia.org/wiki/Parsing).
- The difference between reading and parsing is not just a semantic one: 
    + **reading** is made using (reading) functions that *do not care to understand the formal grammar that underlies HTML* but merely recognize the sequence of symbols included in the HTML file.
    + **parsing** is made employing programs that understand the special meaning of the mark-up structure and reconstructs the implied HTML hierarchy of an HTML file within some R-specified structure.


Getting data (1): **Reading** an HTML file
==========================================

- We can read a file from the web using `readlines()` function:

```{r}
url <- "http://www.r-datacollection.com/materials/html/fortunes.html"
fortunes <- readLines(con = url)
head(fortunes, n=10)
```

- `readLines()` 
    + maps every line of the input file to a separate value in a character vector creating a flat representation of the document.
    + is *agnostic* about the different tag elements (name, attribute, values, etc.) and produces results that do not reflect the documentâ€™s internal hierarchy *as implied by the nested tags* in any sensible way.
    
Getting data (2): Parsing an HTML file
=======================================
- To achieve a useful representation of HTML files, we need to employ a program that 
    + understands the special meaning of the markup structures and 
    + reconstructs the implied hierarchy of an HTML file within some R-specific data structure.
- This representation is also referred to as the *Document Object Model (DOM)*.
    + A Document Object Model is a queryable data object that we can build from
any HTML file and is useful for further processing of document parts.

A distraction: HTML tree structure
==================================
- If one ignores the \<!DOCTYPE html \> tag it is easy to see that an HTML document can be seen as a hierarchichal collection of tags which contain distinct elements.
- This can also be visualised as a tree.
<div align="center"> 
 <img src="images/htmlHierarchy.png" width="60%" style="float:centered"/> 
</div>

- Hint: Copy the source code of the `fortunes.html` document and paste it in [This viewer](https://software.hixie.ch/utilities/js/live-dom-viewer/)


DOM-style parsers
==================
- Transformation from HTML code to the DOM is the task of a *DOM-style parser*. 
- Parsers belong to a *general class of domain-specific programs that traverse over symbol sequences and reconstruct the semantic structure of the document within a data object of the programming environment*.
- Right now there are two mainstream packages that can be used for parsing HTML code
    + [XML package](https://cran.r-project.org/web/packages/XML/index.html) by Duncan Temple and Debbie Nolan,
    + [rvest package](https://github.com/hadley/rvest) by Hadley Wickam,
    + and a few others that one can see at [CRAN Task View: Web Technologies and Services](https://cran.r-project.org/web/views/WebTechnologies.html).
    + See also the table and introduction at: (An R web crawler and scraper)[https://github.com/salimk/Rcrawler]

Scrapping web pages (I): Using the `XML` package
===============================================

- The `XML` package provides an interface to `libxml2` a powerful parsing library written in C.
- The package is designed for two main purposes
    + parsing xml / html content
    + writing xml / html content (*we wonn't cover this*)
    
What can we do with "XML"?
===========================

- We cover 4 major types of tasks that can be performed with "XML"

1. parsing xml / html content
2. obtaining descriptive information about parsed contents
3. navigating the tree structure (ie *accessing its components*)
4. querying and extracting data from parsed contents

- XML package can be used for both XML and HTML parsing. 
- We will see how to use it, later in the course, when we work with XML files.


Scraping web pages (II): Using the `rvest` package
===================================================

- `rvest`is an R package written by the R guru [Hadley Wickam](http://hadley.nz/).
- It is intended to facilitate the process of acquiring data from web pages (not "from the web") and parsing the result into R.
- rvest is inspired to work with [magrittr](https://github.com/tidyverse/magrittr)
- See more information on `rvest` at: 
    + [rvest package on Github](https://github.com/hadley/rvest)
    + [rvest documentation on CRAN](http://cran.r-project.org/web/packages/rvest/index.html)
    + [rstudio blog on rvest](http://blog.rstudio.org/2014/11/24/rvest-easy-web-scraping-with-r/)

Basic rvest capabilities
=======================================

+ Get the data: Create an html document from a url, a file on disk or a string containing html with `read_html()`.
+ Select parts of an html document using css selectors: `html_nodes()` 
    - Read vignette with: `vignette("selectorgadget")` after installing and loading rvest in R.
+ Extract components with 
    - `html_tag()` (the name of the tag),
    - `html_text()` (all text inside the tag), 
    - `html_attr()` (contents of a single attribute) and 
    - `html_attrs()` (all attributes). 
    These are done after using `html_nodes()`.
+ Parse tables into data frames with `html_table()`.
+ Extract, modify and submit forms with `html_form()`, `set_values()` and `submit_form()`.
+ Detect and repair encoding problems with `guess_encoding()` and `repair_encoding()`. Then pass the correct encoding into `html()` as an argument.

`rvest` by the example (1): Grab the data
==========================================

- Imagine we want to extract information from a certain movie on imdb, "The Lego Movie"

[The Lego Movie]("http://www.imdb.com/title/tt1490017/)

```{r eval=FALSE}
# install.packages("rvest", dependencies=TRUE)
require(rvest)
lego_movie <- read_html("http://www.imdb.com/title/tt1490017/")
```

`rvest` by the example (2): parse the data
==========================================

- We start extracting the **movie rating**.
Now we have the data we need to figure out which *selector* matches the data we want.
    + Examining the page with *selectorgadget* we find that it is `strong span`
- We now:
    + use `html_node()` to find the first node that matches that selector, 
    + extract its contents with `html_text()`, and 
    + convert it to numeric with `as.numeric()`

```{r eval=FALSE}
rating <- lego_movie %>% 
  html_node("strong span") %>%
  html_text()# %>%
  # as.numeric()
rating
```

`rvest` by the example (3): keep parsing
==========================================

- We can use a similar process to extract the images:

```{r eval=FALSE}
poster <- lego_movie %>%
  html_nodes(".poster img") %>%
  html_attr("src")
poster
```

`rvest` by the example (4): managing the unexpected
======================================================

- Imagine we want to use a similar process to extract the cast, using `html_nodes()` to find all nodes that match the selector:

    + This example is based on the original "selectorgadget" vignette which used ".itemprop" as selector name to recover the names of the cast.
    + It seems that the wep page may have changed and this selector is not used anymore.
    + The item we look for is within a table so a reasonable selector is "td a", although it returns more contents than desired.
    + An alternative is to use "htmltable()" to recover the table that contains the cast.

Extract cast names using `html_nodes()` and selectors
======================================================

```{r eval=FALSE}
cast <- lego_movie %>%
  html_nodes("td a") %>%
  html_text()
cast
```
Now we need to extract and clean the names.
Notice that we have selected several table's contents. An alternative would have been to start extracting the tables.

```{r eval=FALSE}
library(stringi)
castNames <- cast %>% 
  stri_locate(regex="\\n") 
namesPos<-which(!is.na(castNames[1:44,1]))
names<-cast[namesPos]
castNames2 <- substring(names,2, nchar(names)-1 )
castNames2
```

Extract cast names using `html_tables()`
======================================================
```{r eval=FALSE}
require(rvest)
tables <- html_table(lego_movie, fill = TRUE) #Parses tables into data frames
castTable<- tables[[1]]
castNames3 <- as.character(castTable[-1,2])
castNames3
```


It's your turn now!
====================
- Write a script to find out which actor appears in  higher number of Star War movies.

- Hint: The idea is similar to the previous exercise but with a litlle more work you can 

1. Start in a page that gives you access to the list of Star Wars Films (try googling "Star Wars IMDB")
2. From here write a function to extract the authors list from an IMDB whose URL is providd
3. Apply the function to the URLs list and
4. Tabulate or plot


To learn more ...
==================
- Additional tutorials on web scrapping based on the `rvest` package:
    + https://jorgeromero.net/web-scraping-usando-r/
    + http://zevross.com/blog/2015/05/19/scrape-website-data-with-the-new-r-package-rvest/
    + https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/
    + https://www.datacamp.com/community/tutorials/r-web-scraping-rvest
- The `magrittr` package for executing commands in pipes: [magrittr](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html)
- The `rvest` package: [rvest](https://github.com/hadley/rvest)
- About selectorgadgets
    + [Selectorgadget vignette](https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html)
    + [Selectorgadget.com: The site](http://selectorgadget.com/)
- [A comparative between `XML` and `rvest`](https://www.r-bloggers.com/old-is-new-xml-and-rvest/)
- Scraping exercises: Go to this page for scraping exercises (with solutions, although some of them don't work due to changes in web sites!)
    + [http://www.r-exercises.com/2016/12/20/web-scraping-exercises/](http://www.r-exercises.com/2016/12/20/web-scraping-exercises/)
    



    
